{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "\n",
    "Start Date: Jan 27 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from hyperopt import fmin, tpe,rand, hp, STATUS_OK,space_eval\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "#import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y  X1  X2  X3  X4  X5  X6\n",
       "0  0   3   3   3   4   2   4\n",
       "1  0   3   2   3   5   4   3\n",
       "2  1   5   3   3   3   3   5\n",
       "3  0   5   4   3   3   3   5\n",
       "4  0   5   4   3   3   3   5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/ACME-HappinessSurvey2020.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y     0\n",
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "dtype: int64\n",
      "Y     0\n",
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543\n"
     ]
    }
   ],
   "source": [
    "#seed = random.randint(1000, 9999)\n",
    "seed = 3543#1311 #3717#7739 , 4964, 3717 (best, good recall AND precision), | 3874, 8013 (best acc for bernoulli NB) | 1311 (best for LGBM)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6) (26, 6) (100,) (26,)\n"
     ]
    }
   ],
   "source": [
    "features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6']\n",
    "X = df[features]\n",
    "Y = df['Y']\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 'unhappy' as the positive class\n",
    "def recall_func(y_test, y_pred):\n",
    "    return recall_score(y_test, y_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy Predict helps with setting a baseline.\n",
    "# Running lazy predict multiple times with different random seeds to see the behavior.\n",
    "# Note that a well trained stable model's output should not depend on the seed. Here, the outcome varies because the models are not tuned to the dataset.\n",
    "# This code block is to get an idea about which models would perform best with the dataset.\n",
    "\n",
    "# # Number of iterations\n",
    "# num_iterations = 50\n",
    "\n",
    "# # Dictionary to store top 10 models per iteration\n",
    "# top_models_per_iteration = []\n",
    "\n",
    "# # Dictionary to store best accuracy and corresponding seed for each model\n",
    "# best_seeds = defaultdict(lambda: (0, None))\n",
    "\n",
    "# for _ in range(num_iterations):\n",
    "#     # Split data with different random seed\n",
    "#     seed = random.randint(1000, 9999)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "#     # Initialize LazyClassifier\n",
    "#     clf = LazyClassifier(verbose=0, predictions=False,ignore_warnings=True, custom_metric=recall_func)\n",
    "    \n",
    "#     # Fit and evaluate models\n",
    "#     models, _ = clf.fit(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "#     # Get top 10 models for this iteration\n",
    "#     top_10_models = models.head(5).index.tolist()\n",
    "#     top_models_per_iteration.extend(top_10_models)\n",
    "    \n",
    "#     # Record best accuracy and corresponding seed\n",
    "#     for model_name, row in models.iterrows():\n",
    "#         accuracy = row['recall_func']\n",
    "#         if accuracy > best_seeds[model_name][0]:\n",
    "#             best_seeds[model_name] = (accuracy, seed)\n",
    "\n",
    "# # Count occurrences of each model in the top 10\n",
    "# model_counts = Counter(top_models_per_iteration)\n",
    "\n",
    "# # Get the three most frequently appearing models\n",
    "# best_models = model_counts.most_common(3)\n",
    "\n",
    "# # Display results\n",
    "# print(\"Top 3 Most Frequently Appearing Models in Top 5:\")\n",
    "# for model, count in best_models:\n",
    "#     best_accuracy, best_seed = best_seeds[model]\n",
    "#     print(f\"{model}: {count} times, Best Accuracy: {best_accuracy:.4f}, Best Seed: {best_seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AdaBoostClassifier', <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>), ('BaggingClassifier', <class 'sklearn.ensemble._bagging.BaggingClassifier'>), ('BernoulliNB', <class 'sklearn.naive_bayes.BernoulliNB'>), ('CalibratedClassifierCV', <class 'sklearn.calibration.CalibratedClassifierCV'>), ('CategoricalNB', <class 'sklearn.naive_bayes.CategoricalNB'>), ('DecisionTreeClassifier', <class 'sklearn.tree._classes.DecisionTreeClassifier'>), ('DummyClassifier', <class 'sklearn.dummy.DummyClassifier'>), ('ExtraTreeClassifier', <class 'sklearn.tree._classes.ExtraTreeClassifier'>), ('ExtraTreesClassifier', <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>), ('GaussianNB', <class 'sklearn.naive_bayes.GaussianNB'>), ('KNeighborsClassifier', <class 'sklearn.neighbors._classification.KNeighborsClassifier'>), ('LabelPropagation', <class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>), ('LabelSpreading', <class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>), ('LinearDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>), ('LinearSVC', <class 'sklearn.svm._classes.LinearSVC'>), ('LogisticRegression', <class 'sklearn.linear_model._logistic.LogisticRegression'>), ('NearestCentroid', <class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>), ('NuSVC', <class 'sklearn.svm._classes.NuSVC'>), ('PassiveAggressiveClassifier', <class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>), ('Perceptron', <class 'sklearn.linear_model._perceptron.Perceptron'>), ('QuadraticDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>), ('RandomForestClassifier', <class 'sklearn.ensemble._forest.RandomForestClassifier'>), ('RidgeClassifier', <class 'sklearn.linear_model._ridge.RidgeClassifier'>), ('RidgeClassifierCV', <class 'sklearn.linear_model._ridge.RidgeClassifierCV'>), ('SGDClassifier', <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>), ('SVC', <class 'sklearn.svm._classes.SVC'>), ('StackingClassifier', <class 'sklearn.ensemble._stacking.StackingClassifier'>), ('XGBClassifier', <class 'xgboost.sklearn.XGBClassifier'>), ('LGBMClassifier', <class 'lightgbm.sklearn.LGBMClassifier'>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:02<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 53, number of negative: 47\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.530000 -> initscore=0.120144\n",
      "[LightGBM] [Info] Start training from score 0.120144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>recall_func</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "BernoulliNB                        0.88               0.91     0.91      0.89   \n",
       "AdaBoostClassifier                 0.77               0.76     0.76      0.77   \n",
       "LinearDiscriminantAnalysis         0.73               0.72     0.72      0.73   \n",
       "RidgeClassifierCV                  0.73               0.72     0.72      0.73   \n",
       "RidgeClassifier                    0.73               0.72     0.72      0.73   \n",
       "LogisticRegression                 0.73               0.72     0.72      0.73   \n",
       "LinearSVC                          0.73               0.72     0.72      0.73   \n",
       "NearestCentroid                    0.69               0.71     0.71      0.70   \n",
       "PassiveAggressiveClassifier        0.73               0.71     0.71      0.73   \n",
       "CalibratedClassifierCV             0.73               0.71     0.71      0.73   \n",
       "Perceptron                         0.65               0.70     0.70      0.65   \n",
       "RandomForestClassifier             0.65               0.66     0.66      0.66   \n",
       "GaussianNB                         0.65               0.66     0.66      0.66   \n",
       "LGBMClassifier                     0.65               0.66     0.66      0.66   \n",
       "XGBClassifier                      0.65               0.64     0.64      0.66   \n",
       "BaggingClassifier                  0.62               0.63     0.63      0.62   \n",
       "NuSVC                              0.62               0.63     0.63      0.62   \n",
       "ExtraTreesClassifier               0.62               0.63     0.63      0.62   \n",
       "SGDClassifier                      0.65               0.62     0.62      0.65   \n",
       "LabelSpreading                     0.58               0.60     0.60      0.58   \n",
       "DecisionTreeClassifier             0.58               0.60     0.60      0.58   \n",
       "QuadraticDiscriminantAnalysis      0.58               0.58     0.58      0.58   \n",
       "SVC                                0.58               0.58     0.58      0.58   \n",
       "LabelPropagation                   0.54               0.57     0.57      0.54   \n",
       "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
       "KNeighborsClassifier               0.42               0.40     0.40      0.43   \n",
       "ExtraTreeClassifier                0.38               0.37     0.37      0.39   \n",
       "\n",
       "                               recall_func  Time Taken  \n",
       "Model                                                   \n",
       "BernoulliNB                           1.00        0.02  \n",
       "AdaBoostClassifier                    0.70        0.11  \n",
       "LinearDiscriminantAnalysis            0.70        0.03  \n",
       "RidgeClassifierCV                     0.70        0.02  \n",
       "RidgeClassifier                       0.70        0.03  \n",
       "LogisticRegression                    0.70        0.02  \n",
       "LinearSVC                             0.70        0.02  \n",
       "NearestCentroid                       0.80        0.02  \n",
       "PassiveAggressiveClassifier           0.60        0.02  \n",
       "CalibratedClassifierCV                0.60        0.04  \n",
       "Perceptron                            0.90        0.02  \n",
       "RandomForestClassifier                0.70        0.15  \n",
       "GaussianNB                            0.70        0.02  \n",
       "LGBMClassifier                        0.70        0.18  \n",
       "XGBClassifier                         0.60        1.28  \n",
       "BaggingClassifier                     0.70        0.05  \n",
       "NuSVC                                 0.70        0.02  \n",
       "ExtraTreesClassifier                  0.70        0.12  \n",
       "SGDClassifier                         0.50        0.02  \n",
       "LabelSpreading                        0.70        0.02  \n",
       "DecisionTreeClassifier                0.70        0.02  \n",
       "QuadraticDiscriminantAnalysis         0.60        0.02  \n",
       "SVC                                   0.60        0.02  \n",
       "LabelPropagation                      0.70        0.03  \n",
       "DummyClassifier                       0.00        0.02  \n",
       "KNeighborsClassifier                  0.30        0.03  \n",
       "ExtraTreeClassifier                   0.30        0.02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baselines\n",
    "# LazyClassifier runs all models using their default settings. \n",
    "# Use this to get a sense of the top performing models and then finetune them.\n",
    "# We are more interested predicting the unhappy class (class 0) correctly. So, we make it the positive class.\n",
    "# We should improve recall. So, we select the best models based on recall. \n",
    "# F1 score is a combination of recall and precision. \n",
    "\n",
    "\n",
    "clf = LazyClassifier(verbose=-1,ignore_warnings=True, custom_metric=recall_func, predictions=True, random_state=seed)\n",
    "models,predictions = clf.fit(x_train, x_test, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Sklearn models do not have feature importance function.\n",
    "# So, wrappers for those classifiers were created.\n",
    "class MyBaggingClassifier(BaggingClassifier):\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = np.mean([\n",
    "            tree.feature_importances_ for tree in self.estimators_], axis=0)\n",
    "        \n",
    "        return feature_importances\n",
    "    \n",
    "class MyBernoulliNB(BernoulliNB):\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = np.mean([\n",
    "            tree.feature_importances_ for tree in self.estimators_], axis=0)\n",
    "        \n",
    "        return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing steps\n",
    "preprocessor = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())] #(\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on LazyPredictor results, three classifiers were selected. The three models can be categorized into 3 categories:\n",
    "- BernoulliNB - probabilistic\n",
    "- AdaBoost - ensemble method\n",
    "- LinearSVC - support vector machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.88    1.00       0.77\n",
      "1      0.77    0.70       0.70\n",
      "2      0.73    0.70       0.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-training the selected models\n",
    "\n",
    "# seed = 3543#random.randint(1000, 9999)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "classifiers = {\n",
    "    'NaiveBayes': BernoulliNB(),\n",
    "    'Adaboost': AdaBoostClassifier(random_state=seed),\n",
    "    'SVM': LinearSVC(random_state=seed)\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    model.fit(x_train, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    #importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "\n",
    "# importance_df = pd.DataFrame(\n",
    "#     {\n",
    "#         'Features': features,\n",
    "#         'NaiveBayes': importances[0],\n",
    "#         'LGBM': importances[1],\n",
    "#         'XGBoost': importances[2]\n",
    "#     }\n",
    "# )\n",
    "print(metrics_df)\n",
    "#print(importance_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Since the dataset is small, we can do an exhaustive search of all feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Classifier      Features  Accuracy  Precision  Recall\n",
      "0  NaiveBayes         (X3,)      0.62       0.50    1.00\n",
      "1  NaiveBayes      (X3, X4)      0.62       0.50    1.00\n",
      "2  NaiveBayes  (X1, X3, X5)      0.85       0.71    1.00\n",
      "3    Adaboost  (X1, X2, X3)      0.88       0.82    0.90\n",
      "4    Adaboost  (X1, X2, X5)      0.85       0.75    0.90\n",
      "5    Adaboost         (X1,)      0.85       0.80    0.80\n",
      "6         SVM         (X1,)      0.85       0.80    0.80\n",
      "7         SVM      (X1, X3)      0.85       0.80    0.80\n",
      "8         SVM      (X3, X4)      0.69       0.57    0.80\n"
     ]
    }
   ],
   "source": [
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    best_combinations = []\n",
    "    for i in range(1, len(features)+1):\n",
    "        for combo in combinations(features, i):\n",
    "            X_subset = df[list(combo)]\n",
    "            x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "            model = Pipeline([\n",
    "                (\"preprocessing\", preprocessor),\n",
    "                (\"classifier\", clf)\n",
    "            ])\n",
    "\n",
    "            model.fit(x_train_subset, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(x_test_subset)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "            recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "            best_combinations.append((combo, accuracy, recall, precision))\n",
    "    \n",
    "    best_combinations.sort(key=lambda x:x[2], reverse=True)\n",
    "    results[clf_name] = best_combinations[:3]\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for clf_name, top_combos in results.items():\n",
    "    for combo, acc, recall, precision in top_combos:\n",
    "        final_result.append({'Classifier':clf_name, 'Features':combo, 'Accuracy':acc, 'Precision':precision, 'Recall':recall})\n",
    "\n",
    "final_df = pd.DataFrame(final_result)\n",
    "print(final_df)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "when `importance_getter=='auto'`, the underlying estimator Pipeline should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf_name)\n\u001b[0;32m     12\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFE(model, n_features_to_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Adjust the number of features to select\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Print ranking of features\u001b[39;00m\n\u001b[0;32m     16\u001b[0m ranking \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRanking\u001b[39m\u001b[38;5;124m'\u001b[39m: rfe\u001b[38;5;241m.\u001b[39mranking_})\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:314\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m importances \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_importances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimportance_getter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m ranks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(importances)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# for sparse case ranks is matrix\u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:233\u001b[0m, in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator, getter, transform_func, norm_order)\u001b[0m\n\u001b[0;32m    231\u001b[0m         getter \u001b[38;5;241m=\u001b[39m attrgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen `importance_getter==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`, the underlying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`coef_` or `feature_importances_` attribute. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a fitted estimator to feature selector or call fit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore calling transform.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     getter \u001b[38;5;241m=\u001b[39m attrgetter(getter)\n",
      "\u001b[1;31mValueError\u001b[0m: when `importance_getter=='auto'`, the underlying estimator Pipeline should have `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to feature selector or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    print(clf_name)\n",
    "    rfe = RFE(model, n_features_to_select=4)  # Adjust the number of features to select\n",
    "    rfe.fit(x_train, y_train)\n",
    "\n",
    "    # Print ranking of features\n",
    "    ranking = pd.DataFrame({'Feature': features, 'Ranking': rfe.ranking_})\n",
    "    print(f\"{clf_name} \\n {ranking}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrices for selected combination\n",
    "\n",
    "features_subset = ['X1', 'X3', 'X4', 'X5'] # Common feature set for all three models\n",
    "X_subset = df[features_subset]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    clf.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "   # importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree gives the highest recall and accuracy.\n",
    "XGBoost and ExtraTrees are ensemble models. They could be overfitting. Especially when some features are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check overfitting using ROC curves\n",
    "\n",
    "features_subset = ['X1', 'X3', 'X4', 'X5'] # Common feature set for all three models\n",
    "X_subset = df[features_subset]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    clf.fit(x_train_subset, y_train)\n",
    "\n",
    "    y_probs_train = clf.predict_proba(x_train_subset)[:,0] # Positive class is 0\n",
    "    y_probs_test = clf.predict_proba(x_test_subset)[:,0] # Positive class is 0\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_probs_train, pos_label=0)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_probs_test, pos_label=0)\n",
    "\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    ax[idx].plot(fpr_train, tpr_train, 'b', label='Train AUC = {:0.2f}'.format(roc_auc_train))\n",
    "    ax[idx].plot(fpr_test, tpr_test, 'r', label='Test AUC = {:0.2f}'.format(roc_auc_test))\n",
    "    ax[idx].legend()\n",
    "    ax[idx].set_title(clf_name)\n",
    "    ax[idx].set_xlabel('FPR')\n",
    "    ax[idx].set_ylabel('TPR')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is somewhat evident for XGBoost, but not for ExtraTrees. \n",
    "We need to finetune the models to reduce overfitting.\n",
    "We can use `GridSearchCV` (exhaustively looking at all hyperparameter combinations) here because the dataset is small. If the dataset is large, we can use `RandomSearch`.\n",
    "Note that this is automated hyperparameter tuning. So, cross validation is used here. Cross-validation DOES NOT train the model. It is used to evaluate the current model by fitting the model on part of the training data and VALIDATING on the other part. After validating, if the performance is satisfactory, we fit the model on the entire training set and test on the test partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_scorer(estimator, x, y):\n",
    "    \n",
    "    y_pred = estimator.predict(x)\n",
    "    score = recall_score(y, y_pred, pos_label=0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Finetuning XGBoost to reduce overfitting\n",
    "\n",
    "\n",
    "# features_subset = ['X1', 'X3', 'X4', 'X6'] # Common feature set for all three models\n",
    "# X_subset = df[features_subset]\n",
    "# x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# # A parameter grid for XGBoost\n",
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'gamma': [0, 0.5, 1, 1.5, 2, 5], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'max_depth': [3, 4, 5, 6] #Smaller -> mpre conservative model\n",
    "#         }\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "# kf = KFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "\n",
    "# grid = GridSearchCV(estimator=classifiers['Bagging'], param_grid=params, scoring=my_scorer, n_jobs=4, cv=kf.split(x_train_subset,y_train), verbose=3, error_score='raise')\n",
    "\n",
    "# starttime = timer()\n",
    "# grid.fit(x_train_subset, y_train)\n",
    "# timer(starttime)\n",
    "\n",
    "# print('\\n All results:')\n",
    "# print(grid.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(grid.best_estimator_)\n",
    "# print('\\n Best score:')\n",
    "# print(grid.best_score_ )#print(grid.best_score_ * 2 - 1)\n",
    "# print('\\n Best parameters:')\n",
    "# print(grid.best_params_)\n",
    "# results = pd.DataFrame(grid.cv_results_)\n",
    "# results.to_csv('xgb-grid-search-results-01.csv', index=False)\n",
    "\n",
    "# y_pred = grid.best_estimator_.predict(x_test_subset)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "# recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "# print(f'Accuracy = {accuracy:0.2f}, Precision = {precision:0.2f}, Recall = {recall:0.2f}')\n",
    "\n",
    "# # y_test = grid.best_estimator_.predict_proba(x_test_subset)\n",
    "# # results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n",
    "# # results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clf = XGBClassifier(random_state=seed, gamma = 0.5, max_depth = 3, min_child_weight = 5)\n",
    "#clf = XGBClassifier(random_state=seed, gamma = 0, max_depth = 6, min_child_weight = 1)\n",
    "clf = LGBMClassifier(num_leaves=4, n_estimators=3, min_data_in_leaf=1, verbose=0)\n",
    "\n",
    "# Create a KFold object\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Perform cross-validation\n",
    "#scores = cross_val_score(grid.best_estimator_, x_train_subset, y_train, cv=kfold, scoring=my_scorer)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=kfold, scoring=my_scorer)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Calculate the mean score\n",
    "print(scores.mean())\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax, cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "print(f'Accuracy = {accuracy:0.2f}, Precision = {precision:0.2f}, Recall = {recall:0.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV does not give the best test scores. Default settings give better test scores. But, train cross validation scores vary largely and they are higher for the hyperparameters given by the grid search.\n",
    "High variability of cross validation scores and descrepancy between train and test scores could be because the model has not been fit well or the training set is not generalizing well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold for maintaining class distribution in each fold\n",
    "#skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# Visualize the distributions\n",
    "# fold_number = 1\n",
    "# for train_index, test_index in kf.split(x_train_subset, y_train):\n",
    "#     fold_df = df.iloc[test_index]\n",
    "    \n",
    "#     # Plot class distribution\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "    \n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.countplot(x='Y', data=fold_df)\n",
    "#     plt.title(f'Class Distribution - Fold {fold_number}')\n",
    "#     plt.xlabel('Class')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     # Plot feature distribution (for the first feature as an example)\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.histplot(fold_df['X3'], kde=True, bins=10)\n",
    "#     plt.title(f'Feature Distribution (X3) - Fold {fold_number}')\n",
    "#     plt.xlabel('Feature Value')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'D:/Installations/Graphviz-12.2.1-win64/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the random seed (42 may not give the best results)\n",
    "# Recursive feature elemination (RFE)\n",
    "# HyperOPT - SOTA in hyperparameter tuning\n",
    "# Ensemble models - stacking and voting \n",
    "# Select one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a base classifier for RFE\n",
    "# model = RandomForestClassifier()\n",
    "# rfe = RFE(model, n_features_to_select=2)  # Adjust the number of features to select\n",
    "# rfe.fit(X, y)\n",
    "\n",
    "# Print ranking of features\n",
    "# print(f\"Feature Ranking: {dict(zip(features, rfe.ranking_))}\")\n",
    "\n",
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "\n",
    "    rfe = RFE(clf, n_features_to_select=4)  # Adjust the number of features to select\n",
    "    rfe.fit(x_train, y_train)\n",
    "\n",
    "    # Print ranking of features\n",
    "    #print(f\"Feature Ranking ({clf_name}): {dict(zip(features, rfe.ranking_))}\")\n",
    "    ranking = pd.DataFrame({'Feature': features, 'Ranking': rfe.ranking_})\n",
    "    print(f\"{clf_name} \\n {ranking}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrices for selected combination\n",
    "\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=seed),\n",
    "    'LGBM': LGBMClassifier(random_state=seed), #random_state=seed n_estimators=8, min_data_in_leaf=0\n",
    "    'XGBoost': XGBClassifier(random_state=seed)\n",
    "}\n",
    "\n",
    "features_subset = [['X2', 'X3', 'X4', 'X5'],\n",
    "                   ['X1', 'X3', 'X5', 'X6'],\n",
    "                    ['X1', 'X3', 'X5', 'X6']] # Common feature set for all three models\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "metrics = []\n",
    "importances = []\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()): \n",
    "    X_subset = df[features_subset[idx]]\n",
    "    x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    clf.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    # importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# LGBM optimization with HyperOpt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_subset \u001b[38;5;241m=\u001b[39m df[\u001b[43mfeatures_subset\u001b[49m[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m      4\u001b[0m x_train_subset, x_test_subset, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_subset, Y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define the objective function\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# LGBM optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = LGBMClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 2, 32, 2)),  # Small range to avoid overfitting\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 5, 50, 5)),  # Prevent too small leaves\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),  # 0.001 to 1.0\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3, 5, 7]),  # Limit depth for small dataset\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Feature selection\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Row sampling for regularization\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -4, 1),  # L1 regularization (0.0001 to 10)\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -4, 1),  # L2 regularization (0.0001 to 10)\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Limit estimators for small dataset\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'rf']),  # Try both methods\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = LGBMClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = XGBClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),  # ~0.0001 to 1\n",
    "    'max_depth': hp.choice('max_depth', [2, 3, 4, 5, 6, 7, 8, 9]),  # Integer selection\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Integer 50-300 in steps of 10\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Use 50-100% of data per round\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Use 50-100% of features per tree\n",
    "    'gamma': hp.loguniform('gamma', -5, 1),  # ~0.0067 to 2.71\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, 1),  # L1 regularization ~0.0067 to 2.71\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -5, 2),  # L2 regularization ~0.0067 to 7.39\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = XGBClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "accuracy = final_model.score(x_test_subset,y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241m.\u001b[39mestimator_\u001b[38;5;241m.\u001b[39mget_params()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_model' is not defined"
     ]
    }
   ],
   "source": [
    "final_model.estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
