{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1\n",
    "\n",
    "Start Date: Jan 27 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, LeaveOneOut\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_selection import RFE, RFECV, SelectKBest, mutual_info_classif, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Binarizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from hyperopt import fmin, tpe,rand, hp, STATUS_OK,space_eval, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "#import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Installations\\\\Python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking pythin installation path\n",
    "import os, sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y  X1  X2  X3  X4  X5  X6\n",
       "0  0   3   3   3   4   2   4\n",
       "1  0   3   2   3   5   4   3\n",
       "2  1   5   3   3   3   3   5\n",
       "3  0   5   4   3   3   3   5\n",
       "4  0   5   4   3   3   3   5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"data/ACME-HappinessSurvey2020.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y     0\n",
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "dtype: int64\n",
      "Y     0\n",
      "X1    0\n",
      "X2    0\n",
      "X3    0\n",
      "X4    0\n",
      "X5    0\n",
      "X6    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543\n"
     ]
    }
   ],
   "source": [
    "#seed = random.randint(1000, 9999)\n",
    "seed = 3543#1311 #3717#7739 , 4964, 3717 (best, good recall AND precision), | 3874, 8013 (best acc for bernoulli NB) | 1311 (best for LGBM)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6) (26, 6) (100,) (26,)\n"
     ]
    }
   ],
   "source": [
    "features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6']\n",
    "X = df[features]\n",
    "Y = df['Y']\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting 'unhappy' as the positive class\n",
    "def recall_func(y_test, y_pred):\n",
    "    return recall_score(y_test, y_pred, pos_label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lazy Predict helps with setting a baseline.\n",
    "# # Running lazy predict multiple times with different random seeds to see the behavior.\n",
    "# # Note that a well trained stable model's output should not depend on the seed. Here, the outcome varies because the models are not tuned to the dataset.\n",
    "# # This code block is to get an idea about which models would perform best with the dataset.\n",
    "\n",
    "# # Number of iterations\n",
    "# num_iterations = 50\n",
    "\n",
    "# # Dictionary to store top 10 models per iteration\n",
    "# top_models_per_iteration = []\n",
    "\n",
    "# # Dictionary to store best accuracy and corresponding seed for each model\n",
    "# best_seeds = defaultdict(lambda: (0, None))\n",
    "\n",
    "# for _ in range(num_iterations):\n",
    "#     # Split data with different random seed\n",
    "#     seed = random.randint(1000, 9999)\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "#     # Initialize LazyClassifier\n",
    "#     clf = LazyClassifier(verbose=0, predictions=False,ignore_warnings=True, custom_metric=recall_func)\n",
    "    \n",
    "#     # Fit and evaluate models\n",
    "#     models, _ = clf.fit(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "#     # Get top 10 models for this iteration\n",
    "#     top_10_models = models.head(5).index.tolist()\n",
    "#     top_models_per_iteration.extend(top_10_models)\n",
    "    \n",
    "#     # Record best accuracy and corresponding seed\n",
    "#     for model_name, row in models.iterrows():\n",
    "#         accuracy = row['recall_func']\n",
    "#         if accuracy > best_seeds[model_name][0]:\n",
    "#             best_seeds[model_name] = (accuracy, seed)\n",
    "\n",
    "# # Count occurrences of each model in the top 10\n",
    "# model_counts = Counter(top_models_per_iteration)\n",
    "\n",
    "# # Get the three most frequently appearing models\n",
    "# best_models = model_counts.most_common(3)\n",
    "\n",
    "# # Display results\n",
    "# print(\"Top 3 Most Frequently Appearing Models in Top 5:\")\n",
    "# for model, count in best_models:\n",
    "#     best_accuracy, best_seed = best_seeds[model]\n",
    "#     print(f\"{model}: {count} times, Best Accuracy: {best_accuracy:.4f}, Best Seed: {best_seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AdaBoostClassifier', <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>), ('BaggingClassifier', <class 'sklearn.ensemble._bagging.BaggingClassifier'>), ('BernoulliNB', <class 'sklearn.naive_bayes.BernoulliNB'>), ('CalibratedClassifierCV', <class 'sklearn.calibration.CalibratedClassifierCV'>), ('CategoricalNB', <class 'sklearn.naive_bayes.CategoricalNB'>), ('DecisionTreeClassifier', <class 'sklearn.tree._classes.DecisionTreeClassifier'>), ('DummyClassifier', <class 'sklearn.dummy.DummyClassifier'>), ('ExtraTreeClassifier', <class 'sklearn.tree._classes.ExtraTreeClassifier'>), ('ExtraTreesClassifier', <class 'sklearn.ensemble._forest.ExtraTreesClassifier'>), ('GaussianNB', <class 'sklearn.naive_bayes.GaussianNB'>), ('KNeighborsClassifier', <class 'sklearn.neighbors._classification.KNeighborsClassifier'>), ('LabelPropagation', <class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>), ('LabelSpreading', <class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>), ('LinearDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>), ('LinearSVC', <class 'sklearn.svm._classes.LinearSVC'>), ('LogisticRegression', <class 'sklearn.linear_model._logistic.LogisticRegression'>), ('NearestCentroid', <class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>), ('NuSVC', <class 'sklearn.svm._classes.NuSVC'>), ('PassiveAggressiveClassifier', <class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>), ('Perceptron', <class 'sklearn.linear_model._perceptron.Perceptron'>), ('QuadraticDiscriminantAnalysis', <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>), ('RandomForestClassifier', <class 'sklearn.ensemble._forest.RandomForestClassifier'>), ('RidgeClassifier', <class 'sklearn.linear_model._ridge.RidgeClassifier'>), ('RidgeClassifierCV', <class 'sklearn.linear_model._ridge.RidgeClassifierCV'>), ('SGDClassifier', <class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>), ('SVC', <class 'sklearn.svm._classes.SVC'>), ('StackingClassifier', <class 'sklearn.ensemble._stacking.StackingClassifier'>), ('XGBClassifier', <class 'xgboost.sklearn.XGBClassifier'>), ('LGBMClassifier', <class 'lightgbm.sklearn.LGBMClassifier'>)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:01<00:00, 22.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 53, number of negative: 47\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.530000 -> initscore=0.120144\n",
      "[LightGBM] [Info] Start training from score 0.120144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>recall_func</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "BernoulliNB                        0.88               0.91     0.91      0.89   \n",
       "AdaBoostClassifier                 0.77               0.76     0.76      0.77   \n",
       "LinearDiscriminantAnalysis         0.73               0.72     0.72      0.73   \n",
       "RidgeClassifierCV                  0.73               0.72     0.72      0.73   \n",
       "RidgeClassifier                    0.73               0.72     0.72      0.73   \n",
       "LogisticRegression                 0.73               0.72     0.72      0.73   \n",
       "LinearSVC                          0.73               0.72     0.72      0.73   \n",
       "NearestCentroid                    0.69               0.71     0.71      0.70   \n",
       "PassiveAggressiveClassifier        0.73               0.71     0.71      0.73   \n",
       "CalibratedClassifierCV             0.73               0.71     0.71      0.73   \n",
       "Perceptron                         0.65               0.70     0.70      0.65   \n",
       "RandomForestClassifier             0.65               0.66     0.66      0.66   \n",
       "GaussianNB                         0.65               0.66     0.66      0.66   \n",
       "LGBMClassifier                     0.65               0.66     0.66      0.66   \n",
       "XGBClassifier                      0.65               0.64     0.64      0.66   \n",
       "BaggingClassifier                  0.62               0.63     0.63      0.62   \n",
       "NuSVC                              0.62               0.63     0.63      0.62   \n",
       "ExtraTreesClassifier               0.62               0.63     0.63      0.62   \n",
       "SGDClassifier                      0.65               0.62     0.62      0.65   \n",
       "LabelSpreading                     0.58               0.60     0.60      0.58   \n",
       "DecisionTreeClassifier             0.58               0.60     0.60      0.58   \n",
       "QuadraticDiscriminantAnalysis      0.58               0.58     0.58      0.58   \n",
       "SVC                                0.58               0.58     0.58      0.58   \n",
       "LabelPropagation                   0.54               0.57     0.57      0.54   \n",
       "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
       "KNeighborsClassifier               0.42               0.40     0.40      0.43   \n",
       "ExtraTreeClassifier                0.38               0.37     0.37      0.39   \n",
       "\n",
       "                               recall_func  Time Taken  \n",
       "Model                                                   \n",
       "BernoulliNB                           1.00        0.03  \n",
       "AdaBoostClassifier                    0.70        0.13  \n",
       "LinearDiscriminantAnalysis            0.70        0.01  \n",
       "RidgeClassifierCV                     0.70        0.02  \n",
       "RidgeClassifier                       0.70        0.00  \n",
       "LogisticRegression                    0.70        0.01  \n",
       "LinearSVC                             0.70        0.02  \n",
       "NearestCentroid                       0.80        0.02  \n",
       "PassiveAggressiveClassifier           0.60        0.01  \n",
       "CalibratedClassifierCV                0.60        0.04  \n",
       "Perceptron                            0.90        0.02  \n",
       "RandomForestClassifier                0.70        0.13  \n",
       "GaussianNB                            0.70        0.01  \n",
       "LGBMClassifier                        0.70        0.16  \n",
       "XGBClassifier                         0.60        0.25  \n",
       "BaggingClassifier                     0.70        0.05  \n",
       "NuSVC                                 0.70        0.02  \n",
       "ExtraTreesClassifier                  0.70        0.12  \n",
       "SGDClassifier                         0.50        0.02  \n",
       "LabelSpreading                        0.70        0.02  \n",
       "DecisionTreeClassifier                0.70        0.00  \n",
       "QuadraticDiscriminantAnalysis         0.60        0.02  \n",
       "SVC                                   0.60        0.02  \n",
       "LabelPropagation                      0.70        0.02  \n",
       "DummyClassifier                       0.00        0.00  \n",
       "KNeighborsClassifier                  0.30        0.02  \n",
       "ExtraTreeClassifier                   0.30        0.01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baselines\n",
    "# LazyClassifier runs all models using their default settings. \n",
    "# Use this to get a sense of the top performing models and then finetune them.\n",
    "# We are more interested predicting the unhappy class (class 0) correctly. So, we make it the positive class.\n",
    "# We should improve recall. So, we select the best models based on recall. \n",
    "# F1 score is a combination of recall and precision. \n",
    "\n",
    "\n",
    "clf = LazyClassifier(verbose=-1,ignore_warnings=True, custom_metric=recall_func, predictions=True, random_state=seed)\n",
    "models,predictions = clf.fit(x_train, x_test, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Sklearn models do not have feature importance function.\n",
    "# So, wrappers for those classifiers were created.\n",
    "class MyBaggingClassifier(BaggingClassifier):\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = np.mean([\n",
    "            tree.feature_importances_ for tree in self.estimators_], axis=0)\n",
    "        \n",
    "        return feature_importances\n",
    "    \n",
    "class MyBernoulliNB(BernoulliNB):\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        # Compute feature importance as absolute difference in log probabilities\n",
    "        return np.abs(self.feature_log_prob_[1] - self.feature_log_prob_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing steps\n",
    "preprocessor = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())] #(\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on LazyPredictor results, three classifiers were selected. The three models can be categorized into 3 categories:\n",
    "- BernoulliNB - probabilistic\n",
    "- AdaBoost - ensemble method\n",
    "- LinearSVC - linear model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.88    1.00       0.77\n",
      "1      0.77    0.70       0.70\n",
      "2      0.73    0.70       0.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-training the selected models\n",
    "\n",
    "# seed = 3543#random.randint(1000, 9999)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "classifiers = {\n",
    "    'NaiveBayes': MyBernoulliNB(),#MyBernoulliNB(),\n",
    "    'Adaboost': AdaBoostClassifier(random_state=seed),\n",
    "    'SVM': LinearSVC(random_state=seed)\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    model.fit(x_train, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    #importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "\n",
    "print(metrics_df)\n",
    "#print(importance_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Since the dataset is small, we can do an exhaustive search of all feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining recall with unhappy class as the positive class as the scoring function\n",
    "def my_scorer(estimator, x, y):\n",
    "    \n",
    "    y_pred = estimator.predict(x)\n",
    "    score = recall_score(y, y_pred, pos_label=0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two methods\n",
    "1. Exhaustively searching the whole feature space by considering every feature combination\n",
    "1. Recursive feature elimination with cross validation\n",
    "\n",
    "The first method is in-efficient and time consuming.\n",
    "With the second method, we can use cross validation to determine the optimal number of features required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    best_combinations = []\n",
    "    for i in range(1, len(features)+1):\n",
    "        for combo in combinations(features, i):\n",
    "            X_subset = df[list(combo)]\n",
    "            x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "            model = Pipeline([\n",
    "                (\"preprocessing\", preprocessor),\n",
    "                (\"classifier\", clf)\n",
    "            ])\n",
    "\n",
    "            scores = cross_val_score(model, x_train_subset, y_train, cv=loo, scoring=my_scorer)\n",
    "            avg_score = np.mean(scores)\n",
    "            model.fit(x_train_subset, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(x_test_subset)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "            recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "            best_combinations.append((combo, avg_score, accuracy, recall, precision))\n",
    "    \n",
    "    best_combinations.sort(key=lambda x:x[1], reverse=True)\n",
    "    results[clf_name] = best_combinations[:3]\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for clf_name, top_combos in results.items():\n",
    "    for combo, score, acc, recall, precision in top_combos:\n",
    "        final_result.append({'Classifier':clf_name, 'Features':combo, 'Cross-val Score':score, 'Accuracy':acc, 'Precision':precision, 'Recall':recall})\n",
    "\n",
    "final_df = pd.DataFrame(final_result)\n",
    "print(final_df)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NaiveBayes\n",
      "Optimal Number of Features: 1\n",
      "Selected Features: ['X1']\n",
      "Processing: Adaboost\n",
      "Optimal Number of Features: 6\n",
      "Selected Features: ['X1' 'X2' 'X3' 'X4' 'X5' 'X6']\n",
      "Processing: SVM\n",
      "Optimal Number of Features: 5\n",
      "Selected Features: ['X1' 'X2' 'X3' 'X5' 'X6']\n"
     ]
    }
   ],
   "source": [
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "loo = LeaveOneOut()\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    print(f\"Processing: {clf_name}\")\n",
    "\n",
    "    if clf_name == '888':#'NaiveBayes':\n",
    "        # Use SelectKBest for Naïve Bayes\n",
    "        # Naïve Bayes assumes feature independence. Thus, feature importances may not be suitable for feature selection\n",
    "        feature_selector = SelectKBest(lambda X, y: mutual_info_classif(X, y, random_state=seed), k=4)\n",
    "        model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"feature_selection\", feature_selector),\n",
    "            (\"classifier\", clf)\n",
    "        ])\n",
    "    else:\n",
    "        # Use RFE for other classifiers\n",
    "        #rfe = RFE(clf, n_features_to_select=3)\n",
    "        rfecv = RFECV(estimator=clf, step=1, cv=loo, scoring=my_scorer, n_jobs=-1)\n",
    "        model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"feature_selection\", rfecv),\n",
    "            (\"classifier\", clf)\n",
    "        ])\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Print ranking of features\n",
    "    if clf_name == '888':# 'NaiveBayes':\n",
    "        selected_features = np.array(features)[feature_selector.get_support()]\n",
    "        print(f\"{clf_name} Selected Features: {selected_features}\")\n",
    "    else:\n",
    "        # ranking = pd.DataFrame({'Feature': features, 'Ranking': rfe.ranking_})\n",
    "        # print(f\"{clf_name} \\n {ranking}\")\n",
    "        selected_features = np.array(features)[rfecv.support_]\n",
    "        print(f\"Optimal Number of Features: {rfecv.n_features_}\")\n",
    "        print(f\"Selected Features: {selected_features}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.85    0.80       0.80\n",
      "1      0.77    0.70       0.70\n",
      "2      0.73    0.70       0.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Evaluating feature selection\n",
    "features_subset = [['X1'],#['X1', 'X3', 'X4', 'X5'], #['X2', 'X3', 'X4', 'X6']\n",
    "                   ['X1', 'X2', 'X3', 'X4', 'X5', 'X6'],#['X1', 'X2', 'X3', 'X5', 'X6'], #['X1', 'X2', 'X5', 'X6']\n",
    "                    ['X1', 'X2', 'X3', 'X5', 'X6']] #['X1', 'X3', 'X5', 'X6']\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "metrics = []\n",
    "importances = []\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()): \n",
    "    X_subset = df[features_subset[idx]]\n",
    "    x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", clf)\n",
    "        ])\n",
    "\n",
    "\n",
    "    model.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    # importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping function\n",
    "def early_stop_fn(trials):\n",
    "    threshold=5\n",
    "    if len(trials.trials) < threshold:\n",
    "        return False  # Not enough trials to compare\n",
    "\n",
    "    # Extract loss values from past trials\n",
    "    losses = [t['result']['loss'] for t in trials.trials if 'loss' in t['result']]\n",
    "    \n",
    "    if len(losses) < threshold:\n",
    "        return False  # Ensure we have enough loss values\n",
    "\n",
    "    best_loss_before = min(losses[:-threshold])  # Best loss before recent threshold\n",
    "    best_loss_now = min(losses)  # Current best loss\n",
    "\n",
    "    return best_loss_now >= best_loss_before  # Stop if no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [00:08<06:43,  1.21trial/s, best loss: -0.29]\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 8.24 seconds.\n",
      "Best hyperparameters: {'alpha': 0.16297875749314045, 'binarize': 0.2807385382126685, 'fit_prior': True}\n",
      "   Accuracy  Recall  Precision\n",
      "0      0.85    0.80       0.80\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes hyperparameter tuning with HyperOpt\n",
    "loo = LeaveOneOut()\n",
    "best_params_all = {}\n",
    "\n",
    "X_subset = df[features_subset[0]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    clf = MyBernoulliNB(**params)\n",
    "    model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=loo, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'alpha': hp.loguniform('alpha', -5, 2),  # alpha ∈ (exp(-5), exp(2)) ≈ (0.0067, 7.39)\n",
    "    'fit_prior': hp.choice('fit_prior', [True, False]),  # Boolean choice\n",
    "    'binarize': hp.uniform('binarize', 0, 1)  # Feature binarization threshold\n",
    "}\n",
    "# Run the optimization\n",
    "starttime = timer()\n",
    "best_params = fmin(objective, space, algo=rand.suggest, max_evals=500, early_stop_fn=no_progress_loss(10), rstate=np.random.default_rng(seed))\n",
    "timer(starttime)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "best_params_all['NaiveBayes'] = best_params\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "final_clf = MyBernoulliNB(**best_params)\n",
    "final_model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", final_clf)\n",
    "])\n",
    "\n",
    "\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = final_model.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [03:51<2:13:57, 16.54s/trial, best loss: -0.22]\n",
      "Best hyperparameters: {'algorithm': 'SAMME', 'learning_rate': 0.3502049947378976, 'n_estimators': 50}\n",
      "   Accuracy  Recall  Precision\n",
      "0      0.85    0.80       0.80\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost hyperparameter tuning with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "\n",
    "    # # Create the base estimator separately\n",
    "    # estimator = DecisionTreeClassifier(\n",
    "    #     max_depth=params.pop('max_depth'),\n",
    "    #     min_samples_split=params.pop('min_samples_split'),\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "\n",
    "    clf = AdaBoostClassifier(**params, random_state=seed) #estimator=estimator,\n",
    " \n",
    "    model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=loo, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 200, 10)),  # Number of boosting rounds\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),  # Learning rate (0.0001 to 1)\n",
    "    'algorithm': hp.choice('algorithm', ['SAMME', 'SAMME.R']),  # Type of boosting\n",
    "    #'max_depth': scope.int(hp.quniform('base_max_depth', 1, 30, 1)),  # Depth of weak learners\n",
    "    #'min_samples_split': hp.uniform('base_min_samples_split', 0.01, 0.5),  # Min samples per split\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500, early_stop_fn=no_progress_loss(10), rstate=np.random.default_rng(seed))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "best_params_all['AdaBoost'] = best_params\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "#best_params['max_depth'] = int(best_params['max_depth'])\n",
    "\n",
    "# estimator = DecisionTreeClassifier(\n",
    "#         max_depth=best_params.pop('max_depth'),\n",
    "#         min_samples_split=best_params.pop('min_samples_split'),\n",
    "#         random_state=seed\n",
    "#     )\n",
    "\n",
    "final_clf = AdaBoostClassifier(**best_params, random_state=seed) #estimator=estimator, \n",
    "\n",
    "final_model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", final_clf)\n",
    "])\n",
    "\n",
    "\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = final_model.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/500 [00:14<06:17,  1.27trial/s, best loss: -0.2]\n",
      "Best hyperparameters: {'C': 0.013433227072091682, 'loss': 'hinge', 'max_iter': 4500, 'tol': 0.0500466063883078}\n",
      "   Accuracy  Recall  Precision\n",
      "0      0.69    0.60       0.60\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC hyperparameter tuning with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[2]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    clf = LinearSVC(**params, random_state=seed)\n",
    "    model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=loo, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'C': hp.loguniform('C', -5, 2),  # Regularization strength (10^-5 to 10^2)\n",
    "    'loss': hp.choice('loss', ['hinge', 'squared_hinge']),  # Loss function\n",
    "    'tol': hp.loguniform('tol', -6, -2),  # Stopping tolerance (10^-6 to 10^-2)\n",
    "    'max_iter': scope.int(hp.quniform('max_iter', 500, 5000, 100)),  # Number of iterations\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500, early_stop_fn=no_progress_loss(10), rstate=np.random.default_rng(seed)) #rand\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "best_params_all['SVM'] = best_params\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "\n",
    "best_params['max_iter'] = int(best_params['max_iter'])\n",
    "\n",
    "final_clf = LinearSVC(**best_params, random_state=seed)\n",
    "final_model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", final_clf)\n",
    "])\n",
    "\n",
    "\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = final_model.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.85    0.80       0.80\n",
      "1      0.85    0.80       0.80\n",
      "2      0.69    0.60       0.60\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-training the selected models\n",
    "\n",
    "seed = 3543 #random.randint(1000, 9999) #\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "classifiers = {\n",
    "    'NaiveBayes': MyBernoulliNB(**best_params_all['NaiveBayes']),#MyBernoulliNB(),\n",
    "    'Adaboost': AdaBoostClassifier(**best_params_all['AdaBoost'], random_state=seed),\n",
    "    'SVM': LinearSVC(**best_params_all['SVM'], random_state=seed)\n",
    "}\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    X_subset = df[features_subset[idx]]\n",
    "    x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    model = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", clf)\n",
    "    ])\n",
    "\n",
    "    model.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    #importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "\n",
    "# importance_df = pd.DataFrame(\n",
    "#     {\n",
    "#         'Features': features,\n",
    "#         'NaiveBayes': importances[0],\n",
    "#         'LGBM': importances[1],\n",
    "#         'XGBoost': importances[2]\n",
    "#     }\n",
    "# )\n",
    "print(metrics_df)\n",
    "#print(importance_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Mean Accuracy  Std Accuracy  Mean Recall  Std Recall\n",
      "1    Adaboost           0.57          0.09         0.51        0.15\n",
      "0  NaiveBayes           0.58          0.10         0.49        0.13\n",
      "2         SVM           0.55          0.08         0.40        0.11\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'NaiveBayes': MyBernoulliNB(**best_params_all['NaiveBayes']),#MyBernoulliNB(),\n",
    "    'Adaboost': AdaBoostClassifier(**best_params_all['AdaBoost'], random_state=seed),\n",
    "    'SVM': LinearSVC(**best_params_all['SVM'], random_state=seed)\n",
    "}\n",
    "\n",
    "# Number of trials\n",
    "num_trials = 20  \n",
    "performance_results = {clf: [] for clf in classifiers}\n",
    "\n",
    "# Run multiple trials with different seeds\n",
    "for _ in range(num_trials):\n",
    "    seed = random.randint(1000, 9999)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "\n",
    "        model = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", clf)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        model.fit(x_train, y_train)  # Train with fixed hyperparameters\n",
    "        y_pred = model.predict(x_test)\n",
    "\n",
    "        # Compute performance metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, pos_label=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, pos_label=0),\n",
    "        }\n",
    "        \n",
    "        performance_results[clf_name].append(metrics)\n",
    "\n",
    "# Compute mean and standard deviation for each model\n",
    "summary_results = []\n",
    "for clf_name, metrics_list in performance_results.items():\n",
    "    df_metrics = pd.DataFrame(metrics_list)\n",
    "    mean_metrics = df_metrics.mean()\n",
    "    std_metrics = df_metrics.std()\n",
    "    \n",
    "    summary_results.append({\n",
    "        \"Model\": clf_name,\n",
    "        \"Mean Accuracy\": mean_metrics[\"accuracy\"],\n",
    "        \"Std Accuracy\": std_metrics[\"accuracy\"],\n",
    "        \"Mean Recall\": mean_metrics[\"recall\"],\n",
    "        \"Std Recall\": std_metrics[\"recall\"]\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(summary_results)\n",
    "df_results = df_results.sort_values(by=\"Mean Recall\", ascending=False)\n",
    "\n",
    "# Display final ranking of models\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting and stacking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.85    0.90       0.75\n",
      "1      0.88    0.90       0.82\n",
      "2      0.88    0.90       0.82\n"
     ]
    }
   ],
   "source": [
    "# clf1 = MyBernoulliNB(**best_params_all['NaiveBayes'])\n",
    "# clf2 = AdaBoostClassifier(**best_params_all['AdaBoost'], random_state=seed)\n",
    "# clf3 = LinearSVC(**best_params_all['SVM'], random_state=seed)\n",
    "\n",
    "seed = seed = 3543#random.randint(1000,9999)\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "y_preds = {}\n",
    "\n",
    "# Hard voting \n",
    "clf1 = MyBernoulliNB()\n",
    "clf2 = AdaBoostClassifier(random_state=seed)\n",
    "clf3 = SVC(random_state=seed, probability=True)\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[('nb', clf1), ('aboost', clf2), ('lnsvc', clf3)],voting='hard')\n",
    "\n",
    "model1 = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", eclf1)\n",
    "])\n",
    "\n",
    "model1.fit(x_train, y_train)\n",
    "y_preds['hard'] = model1.predict(x_test)\n",
    "\n",
    "# Soft voting\n",
    "clf1 = MyBernoulliNB()\n",
    "clf2 = AdaBoostClassifier(random_state=seed)\n",
    "clf3 = SVC(random_state=seed, probability=True)\n",
    "\n",
    "eclf2 = VotingClassifier(estimators=[('nb', clf1), ('aboost', clf2), ('lnsvc', clf3)], voting='soft')\n",
    "\n",
    "model2 = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", eclf2)\n",
    "])\n",
    "\n",
    "model2.fit(x_train, y_train)\n",
    "y_preds['soft'] = model2.predict(x_test)\n",
    "\n",
    "# Soft voting with weights\n",
    "clf1 = MyBernoulliNB()\n",
    "clf2 = AdaBoostClassifier(random_state=seed)\n",
    "clf3 = SVC(random_state=seed, probability=True)\n",
    "\n",
    "eclf3 = VotingClassifier(estimators=[('nb', clf1), ('aboost', clf2), ('lnsvc', clf3)], voting='soft', weights=[2, 1, 1])\n",
    "\n",
    "model3 = Pipeline([\n",
    "            (\"preprocessing\", preprocessor),\n",
    "            (\"classifier\", eclf3)\n",
    "])\n",
    "\n",
    "model3.fit(x_train, y_train)\n",
    "y_preds['weighted'] = model3.predict(x_test)\n",
    "\n",
    "# Print accuracy scores\n",
    "metrics = []\n",
    "for model_name, preds in y_preds.items():\n",
    "\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    precision = precision_score(y_test, preds, pos_label=0)\n",
    "    recall = recall_score(y_test, preds, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n",
    "\n",
    "# for clf, label in zip([clf1, clf2, clf3, eclf], ['Naive Bayes', 'AdaBoost ', 'LinearSVC', 'Ensemble']):\n",
    "#     scores = cross_val_score(clf, x_train, y_train, scoring='accuracy', cv=5)\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.88    0.90       0.82\n"
     ]
    }
   ],
   "source": [
    "# Stacking classifier\n",
    "\n",
    "clf1 = MyBernoulliNB()\n",
    "clf2 = AdaBoostClassifier(random_state=seed)\n",
    "clf3 = SVC(random_state=seed, probability=True)\n",
    "\n",
    "# Define the base models\n",
    "estimators = [('nb', clf1), ('aboost', clf2), ('lnsvc', clf3)]\n",
    "final_estimator = XGBClassifier(random_state=seed)\n",
    "\n",
    "eclf4 = StackingClassifier(estimators=estimators,final_estimator=final_estimator,cv=6)\n",
    "eclf4.fit(x_train, y_train)\n",
    "y_pred = eclf4.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision = precision_score(y_test, preds, pos_label=0)\n",
    "recall = recall_score(y_test, preds, pos_label=0)\n",
    "\n",
    "metrics = [[accuracy, recall, precision]]\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Finetuning XGBoost to reduce overfitting\n",
    "\n",
    "\n",
    "# features_subset = ['X1', 'X3', 'X4', 'X6'] # Common feature set for all three models\n",
    "# X_subset = df[features_subset]\n",
    "# x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# # A parameter grid for XGBoost\n",
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'gamma': [0, 0.5, 1, 1.5, 2, 5], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'max_depth': [3, 4, 5, 6] #Smaller -> mpre conservative model\n",
    "#         }\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "# kf = KFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "\n",
    "# grid = GridSearchCV(estimator=classifiers['Bagging'], param_grid=params, scoring=my_scorer, n_jobs=4, cv=kf.split(x_train_subset,y_train), verbose=3, error_score='raise')\n",
    "\n",
    "# starttime = timer()\n",
    "# grid.fit(x_train_subset, y_train)\n",
    "# timer(starttime)\n",
    "\n",
    "# print('\\n All results:')\n",
    "# print(grid.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(grid.best_estimator_)\n",
    "# print('\\n Best score:')\n",
    "# print(grid.best_score_ )#print(grid.best_score_ * 2 - 1)\n",
    "# print('\\n Best parameters:')\n",
    "# print(grid.best_params_)\n",
    "# results = pd.DataFrame(grid.cv_results_)\n",
    "# results.to_csv('xgb-grid-search-results-01.csv', index=False)\n",
    "\n",
    "# y_pred = grid.best_estimator_.predict(x_test_subset)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "# recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "# print(f'Accuracy = {accuracy:0.2f}, Precision = {precision:0.2f}, Recall = {recall:0.2f}')\n",
    "\n",
    "# # y_test = grid.best_estimator_.predict_proba(x_test_subset)\n",
    "# # results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n",
    "# # results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV does not give the best test scores. Default settings give better test scores. But, train cross validation scores vary largely and they are higher for the hyperparameters given by the grid search.\n",
    "High variability of cross validation scores and descrepancy between train and test scores could be because the model has not been fit well or the training set is not generalizing well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold for maintaining class distribution in each fold\n",
    "#skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# Visualize the distributions\n",
    "# fold_number = 1\n",
    "# for train_index, test_index in kf.split(x_train_subset, y_train):\n",
    "#     fold_df = df.iloc[test_index]\n",
    "    \n",
    "#     # Plot class distribution\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "    \n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.countplot(x='Y', data=fold_df)\n",
    "#     plt.title(f'Class Distribution - Fold {fold_number}')\n",
    "#     plt.xlabel('Class')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     # Plot feature distribution (for the first feature as an example)\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.histplot(fold_df['X3'], kde=True, bins=10)\n",
    "#     plt.title(f'Feature Distribution (X3) - Fold {fold_number}')\n",
    "#     plt.xlabel('Feature Value')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = LGBMClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 2, 32, 2)),  # Small range to avoid overfitting\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 5, 50, 5)),  # Prevent too small leaves\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),  # 0.001 to 1.0\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3, 5, 7]),  # Limit depth for small dataset\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Feature selection\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Row sampling for regularization\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -4, 1),  # L1 regularization (0.0001 to 10)\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -4, 1),  # L2 regularization (0.0001 to 10)\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Limit estimators for small dataset\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'rf']),  # Try both methods\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = LGBMClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = XGBClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),  # ~0.0001 to 1\n",
    "    'max_depth': hp.choice('max_depth', [2, 3, 4, 5, 6, 7, 8, 9]),  # Integer selection\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Integer 50-300 in steps of 10\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Use 50-100% of data per round\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Use 50-100% of features per tree\n",
    "    'gamma': hp.loguniform('gamma', -5, 1),  # ~0.0067 to 2.71\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, 1),  # L1 regularization ~0.0067 to 2.71\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -5, 2),  # L2 regularization ~0.0067 to 7.39\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = XGBClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "accuracy = final_model.score(x_test_subset,y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree gives the highest recall and accuracy.\n",
    "XGBoost and ExtraTrees are ensemble models. They could be overfitting. Especially when some features are removed. \n",
    "\n",
    "Overfitting is somewhat evident for XGBoost, but not for ExtraTrees. \n",
    "We need to finetune the models to reduce overfitting.\n",
    "We can use `GridSearchCV` (exhaustively looking at all hyperparameter combinations) here because the dataset is small. If the dataset is large, we can use `RandomSearch`.\n",
    "Note that this is automated hyperparameter tuning. So, cross validation is used here. Cross-validation DOES NOT train the model. It is used to evaluate the current model by fitting the model on part of the training data and VALIDATING on the other part. After validating, if the performance is satisfactory, we fit the model on the entire training set and test on the test partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check overfitting using ROC curves\n",
    "\n",
    "features_subset = ['X1', 'X3', 'X4', 'X5'] # Common feature set for all three models\n",
    "X_subset = df[features_subset]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    clf.fit(x_train_subset, y_train)\n",
    "\n",
    "    y_probs_train = clf.predict_proba(x_train_subset)[:,0] # Positive class is 0\n",
    "    y_probs_test = clf.predict_proba(x_test_subset)[:,0] # Positive class is 0\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_probs_train, pos_label=0)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_probs_test, pos_label=0)\n",
    "\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    ax[idx].plot(fpr_train, tpr_train, 'b', label='Train AUC = {:0.2f}'.format(roc_auc_train))\n",
    "    ax[idx].plot(fpr_test, tpr_test, 'r', label='Test AUC = {:0.2f}'.format(roc_auc_test))\n",
    "    ax[idx].legend()\n",
    "    ax[idx].set_title(clf_name)\n",
    "    ax[idx].set_xlabel('FPR')\n",
    "    ax[idx].set_ylabel('TPR')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
