{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: Jan 27 2025\n",
    "\n",
    "Classification models for Project 1\n",
    "\n",
    "Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from hyperopt import fmin, tpe,rand, hp, STATUS_OK,space_eval\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from xgboost import plot_tree\n",
    "#import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a few seeds\n",
    "# Recursive Feature Elimination\n",
    "# HyperOpt\n",
    "# Ensemble models -> stacking and voting (must choose a set of estimators) May not give better performance compared to single models. But just try them out.\n",
    "# Then, select one model (to wrap up the project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "df = pd.read_csv(\"ACME-HappinessSurvey2020.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#seed = random.randint(1000, 9999)\n",
    "seed = 2851#1311 #3717#7739 , 4964, 3717 (best, good recall AND precision), | 3874, 8013 (best acc for bernoulli NB) | 1311 (best for LGBM)\n",
    "print(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6']\n",
    "X = df[features]\n",
    "Y = df['Y']\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_func(y_test, y_pred):\n",
    "    return recall_score(y_test, y_pred, pos_label=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 50\n",
    "\n",
    "# Dictionary to store top 10 models per iteration\n",
    "top_models_per_iteration = []\n",
    "\n",
    "# Dictionary to store best accuracy and corresponding seed for each model\n",
    "best_seeds = defaultdict(lambda: (0, None))\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Split data with different random seed\n",
    "    seed = random.randint(1000, 9999)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Initialize LazyClassifier\n",
    "    clf = LazyClassifier(verbose=0, predictions=False,ignore_warnings=True, custom_metric=recall_func,)\n",
    "    \n",
    "    # Fit and evaluate models\n",
    "    models, _ = clf.fit(x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    # Get top 10 models for this iteration\n",
    "    top_10_models = models.head(5).index.tolist()\n",
    "    top_models_per_iteration.extend(top_10_models)\n",
    "    \n",
    "    # Record best accuracy and corresponding seed\n",
    "    for model_name, row in models.iterrows():\n",
    "        accuracy = row['recall_func']\n",
    "        if accuracy > best_seeds[model_name][0]:\n",
    "            best_seeds[model_name] = (accuracy, seed)\n",
    "\n",
    "# Count occurrences of each model in the top 10\n",
    "model_counts = Counter(top_models_per_iteration)\n",
    "\n",
    "# Get the three most frequently appearing models\n",
    "best_models = model_counts.most_common(3)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 3 Most Frequently Appearing Models in Top 5:\")\n",
    "for model, count in best_models:\n",
    "    best_accuracy, best_seed = best_seeds[model]\n",
    "    print(f\"{model}: {count} times, Best Accuracy: {best_accuracy:.4f}, Best Seed: {best_seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baselines\n",
    "# LazyClassifier runs all models using their default settings. \n",
    "# Use this to get a sense of the top performing models and then finetune them.\n",
    "# We are more interested predicting the unhappy class (class 0) correctly. So, we make it the positive class.\n",
    "# We should improve recall. So, we select the best models based on recall. \n",
    "# F1 score is a combination of recall and precision. \n",
    "\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=recall_func, predictions=True, random_state=seed)\n",
    "models,predictions = clf.fit(x_train, x_test, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features and fitting the models.\n",
    "# Scaling does not change the results.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "x_train_scaled, x_test_scaled, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(x_train_scaled.shape, x_test_scaled.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Baselines\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=recall_func, predictions=True)\n",
    "models,predictions = clf.fit(x_train_scaled, x_test_scaled, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on selected classifiers\n",
    "\n",
    "1. XGBoost (Extreme gradient boosting)\n",
    "\n",
    "    Highly optimized gradient boosting using decision trees as weak learners.\n",
    "\n",
    "Gini impurity - Used by tree classifiers to decide the best split at each node. The split with the lowest impurity is chosen. \n",
    "Feature importance - The amount by which a feature decreases the Gini impurity at each node. The higher the better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn BaggingClassifier does not have feature importance function.\n",
    "# So, a wrapper for BaggingClassifier was created.\n",
    "class MyBaggingClassifier(BaggingClassifier):\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        feature_importances = np.mean([\n",
    "            tree.feature_importances_ for tree in self.estimators_], axis=0)\n",
    "        \n",
    "        return feature_importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy  Recall  Precision\n",
      "0      0.58    0.67       0.53\n",
      "1      0.65    1.00       0.57\n",
      "2      0.62    0.75       0.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'LGBM': LGBMClassifier(num_leaves=3), #random_state=seed\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    clf.fit(x_train, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    #importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "\n",
    "# importance_df = pd.DataFrame(\n",
    "#     {\n",
    "#         'Features': features,\n",
    "#         'NaiveBayes': importances[0],\n",
    "#         'LGBM': importances[1],\n",
    "#         'XGBoost': importances[2]\n",
    "#     }\n",
    "# )\n",
    "print(metrics_df)\n",
    "#print(importance_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the importance values, X1 shows the highest importance for all three classifiers, but other feature importances don't show a pattern among the classifiers.\n",
    "\n",
    "What other ways to see how each feature contribute to the model performance? Since the dataset is small, we can do an exhaustive search of all feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    best_combinations = []\n",
    "    for i in range(1, len(features)+1):\n",
    "        for combo in combinations(features, i):\n",
    "            X_subset = df[list(combo)]\n",
    "            x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "            clf.fit(x_train_subset, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = clf.predict(x_test_subset)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "            recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "            best_combinations.append((combo, accuracy, recall, precision))\n",
    "    \n",
    "    best_combinations.sort(key=lambda x:x[2], reverse=True)\n",
    "    results[clf_name] = best_combinations[:3]\n",
    "\n",
    "final_result = []\n",
    "\n",
    "for clf_name, top_combos in results.items():\n",
    "    for combo, acc, recall, precision in top_combos:\n",
    "        final_result.append({'Classifier':clf_name, 'Features':combo, 'Accuracy':acc, 'Precision':precision, 'Recall':recall})\n",
    "\n",
    "final_df = pd.DataFrame(final_result)\n",
    "print(final_df)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(X1, X3, X4, X6) combination is common to all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrices for selected combination\n",
    "\n",
    "features_subset = ['X1', 'X3', 'X4', 'X5'] # Common feature set for all three models\n",
    "X_subset = df[features_subset]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "metrics = []\n",
    "importances = []\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "\n",
    "    clf.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "   # importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree gives the highest recall and accuracy.\n",
    "XGBoost and ExtraTrees are ensemble models. They could be overfitting. Especially when some features are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check overfitting using ROC curves\n",
    "\n",
    "features_subset = ['X1', 'X3', 'X4', 'X5'] # Common feature set for all three models\n",
    "X_subset = df[features_subset]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    clf.fit(x_train_subset, y_train)\n",
    "\n",
    "    y_probs_train = clf.predict_proba(x_train_subset)[:,0] # Positive class is 0\n",
    "    y_probs_test = clf.predict_proba(x_test_subset)[:,0] # Positive class is 0\n",
    "\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_probs_train, pos_label=0)\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_probs_test, pos_label=0)\n",
    "\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    ax[idx].plot(fpr_train, tpr_train, 'b', label='Train AUC = {:0.2f}'.format(roc_auc_train))\n",
    "    ax[idx].plot(fpr_test, tpr_test, 'r', label='Test AUC = {:0.2f}'.format(roc_auc_test))\n",
    "    ax[idx].legend()\n",
    "    ax[idx].set_title(clf_name)\n",
    "    ax[idx].set_xlabel('FPR')\n",
    "    ax[idx].set_ylabel('TPR')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is somewhat evident for XGBoost, but not for ExtraTrees. \n",
    "We need to finetune the models to reduce overfitting.\n",
    "We can use `GridSearchCV` (exhaustively looking at all hyperparameter combinations) here because the dataset is small. If the dataset is large, we can use `RandomSearch`.\n",
    "Note that this is automated hyperparameter tuning. So, cross validation is used here. Cross-validation DOES NOT train the model. It is used to evaluate the current model by fitting the model on part of the training data and VALIDATING on the other part. After validating, if the performance is satisfactory, we fit the model on the entire training set and test on the test partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_scorer(estimator, x, y):\n",
    "    \n",
    "    y_pred = estimator.predict(x)\n",
    "    score = recall_score(y, y_pred, pos_label=0)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Finetuning XGBoost to reduce overfitting\n",
    "\n",
    "\n",
    "# features_subset = ['X1', 'X3', 'X4', 'X6'] # Common feature set for all three models\n",
    "# X_subset = df[features_subset]\n",
    "# x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# # A parameter grid for XGBoost\n",
    "# params = {\n",
    "#         'min_child_weight': [1, 5, 10], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'gamma': [0, 0.5, 1, 1.5, 2, 5], #Larger -> more conservative model (i.e. less splitting)\n",
    "#         'max_depth': [3, 4, 5, 6] #Smaller -> mpre conservative model\n",
    "#         }\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "# kf = KFold(n_splits=10, shuffle = True, random_state = seed)\n",
    "\n",
    "# grid = GridSearchCV(estimator=classifiers['Bagging'], param_grid=params, scoring=my_scorer, n_jobs=4, cv=kf.split(x_train_subset,y_train), verbose=3, error_score='raise')\n",
    "\n",
    "# starttime = timer()\n",
    "# grid.fit(x_train_subset, y_train)\n",
    "# timer(starttime)\n",
    "\n",
    "# print('\\n All results:')\n",
    "# print(grid.cv_results_)\n",
    "# print('\\n Best estimator:')\n",
    "# print(grid.best_estimator_)\n",
    "# print('\\n Best score:')\n",
    "# print(grid.best_score_ )#print(grid.best_score_ * 2 - 1)\n",
    "# print('\\n Best parameters:')\n",
    "# print(grid.best_params_)\n",
    "# results = pd.DataFrame(grid.cv_results_)\n",
    "# results.to_csv('xgb-grid-search-results-01.csv', index=False)\n",
    "\n",
    "# y_pred = grid.best_estimator_.predict(x_test_subset)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "# recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "# print(f'Accuracy = {accuracy:0.2f}, Precision = {precision:0.2f}, Recall = {recall:0.2f}')\n",
    "\n",
    "# # y_test = grid.best_estimator_.predict_proba(x_test_subset)\n",
    "# # results_df = pd.DataFrame(data={'id':test_df['id'], 'target':y_test[:,1]})\n",
    "# # results_df.to_csv('submission-grid-search-xgb-porto-01.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clf = XGBClassifier(random_state=seed, gamma = 0.5, max_depth = 3, min_child_weight = 5)\n",
    "#clf = XGBClassifier(random_state=seed, gamma = 0, max_depth = 6, min_child_weight = 1)\n",
    "clf = LGBMClassifier(num_leaves=4, n_estimators=3, min_data_in_leaf=1, verbose=0)\n",
    "\n",
    "# Create a KFold object\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "# Perform cross-validation\n",
    "#scores = cross_val_score(grid.best_estimator_, x_train_subset, y_train, cv=kfold, scoring=my_scorer)\n",
    "scores = cross_val_score(clf, x_train, y_train, cv=kfold, scoring=my_scorer)\n",
    "\n",
    "# Print the scores\n",
    "print(scores)\n",
    "\n",
    "# Calculate the mean score\n",
    "print(scores.mean())\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax, cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "print(f'Accuracy = {accuracy:0.2f}, Precision = {precision:0.2f}, Recall = {recall:0.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV does not give the best test scores. Default settings give better test scores. But, train cross validation scores vary largely and they are higher for the hyperparameters given by the grid search.\n",
    "High variability of cross validation scores and descrepancy between train and test scores could be because the model has not been fit well or the training set is not generalizing well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StratifiedKFold for maintaining class distribution in each fold\n",
    "#skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# Visualize the distributions\n",
    "# fold_number = 1\n",
    "# for train_index, test_index in kf.split(x_train_subset, y_train):\n",
    "#     fold_df = df.iloc[test_index]\n",
    "    \n",
    "#     # Plot class distribution\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "    \n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     sns.countplot(x='Y', data=fold_df)\n",
    "#     plt.title(f'Class Distribution - Fold {fold_number}')\n",
    "#     plt.xlabel('Class')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     # Plot feature distribution (for the first feature as an example)\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     sns.histplot(fold_df['X3'], kde=True, bins=10)\n",
    "#     plt.title(f'Feature Distribution (X3) - Fold {fold_number}')\n",
    "#     plt.xlabel('Feature Value')\n",
    "#     plt.ylabel('Count')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     fold_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] += os.pathsep + 'D:/Installations/Graphviz-12.2.1-win64/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with the random seed (42 may not give the best results)\n",
    "# Recursive feature elemination (RFE)\n",
    "# HyperOPT - SOTA in hyperparameter tuning\n",
    "# Ensemble models - stacking and voting \n",
    "# Select one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest \n",
      "   Feature  Ranking\n",
      "0      X1        2\n",
      "1      X2        1\n",
      "2      X3        1\n",
      "3      X4        1\n",
      "4      X5        1\n",
      "5      X6        3\n",
      "LGBM \n",
      "   Feature  Ranking\n",
      "0      X1        1\n",
      "1      X2        2\n",
      "2      X3        1\n",
      "3      X4        3\n",
      "4      X5        1\n",
      "5      X6        1\n",
      "XGBoost \n",
      "   Feature  Ranking\n",
      "0      X1        1\n",
      "1      X2        3\n",
      "2      X3        1\n",
      "3      X4        2\n",
      "4      X5        1\n",
      "5      X6        1\n"
     ]
    }
   ],
   "source": [
    "# Use a base classifier for RFE\n",
    "# model = RandomForestClassifier()\n",
    "# rfe = RFE(model, n_features_to_select=2)  # Adjust the number of features to select\n",
    "# rfe.fit(X, y)\n",
    "\n",
    "# Print ranking of features\n",
    "# print(f\"Feature Ranking: {dict(zip(features, rfe.ranking_))}\")\n",
    "\n",
    "## Feature-wise classification\n",
    "results = {clf_name:[] for clf_name in classifiers.keys()}\n",
    "\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    rfe = RFE(clf, n_features_to_select=4)  # Adjust the number of features to select\n",
    "    rfe.fit(x_train, y_train)\n",
    "\n",
    "    # Print ranking of features\n",
    "    #print(f\"Feature Ranking ({clf_name}): {dict(zip(features, rfe.ranking_))}\")\n",
    "    ranking = pd.DataFrame({'Feature': features, 'Ranking': rfe.ranking_})\n",
    "    print(f\"{clf_name} \\n {ranking}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "   Accuracy  Recall  Precision\n",
      "0      0.58    0.50       0.55\n",
      "1      0.58    0.92       0.52\n",
      "2      0.65    0.83       0.59\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Confusion matrices for selected combination\n",
    "\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=seed),\n",
    "    'LGBM': LGBMClassifier(random_state=seed), #random_state=seed n_estimators=8, min_data_in_leaf=0\n",
    "    'XGBoost': XGBClassifier(random_state=seed)\n",
    "}\n",
    "\n",
    "features_subset = [['X2', 'X3', 'X4', 'X5'],\n",
    "                   ['X1', 'X3', 'X5', 'X6'],\n",
    "                    ['X1', 'X3', 'X5', 'X6']] # Common feature set for all three models\n",
    "fig, ax = plt.subplots(1, 3, figsize=(9,3), constrained_layout=True)\n",
    "metrics = []\n",
    "importances = []\n",
    "for idx, (clf_name, clf) in enumerate(classifiers.items()): \n",
    "    X_subset = df[features_subset[idx]]\n",
    "    x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    clf.fit(x_train_subset, y_train) #clf.fit(x_train, y_train, eval_set=[(x_test, y_test)]) #(eval_set for early stopping)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "    metrics.append([accuracy, recall, precision])\n",
    "\n",
    "    # importances.append(clf.feature_importances_)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "    s.set_xlabel('Pred')\n",
    "    s.set_ylabel('True')\n",
    "    s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.path.dirname(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [00:55<00:00,  8.96trial/s, best loss: -1.0]              \n",
      "Best hyperparameters: {'boosting_type': 'rf', 'colsample_bytree': 0.8574169608312157, 'learning_rate': 0.08137210859279474, 'max_depth': 7, 'min_data_in_leaf': 35, 'n_estimators': 290, 'num_leaves': 26, 'reg_alpha': 0.6550304486011378, 'reg_lambda': 0.3749302997561087, 'subsample': 0.6993777723871315}\n",
      "   Accuracy  Recall  Precision\n",
      "0      0.65    0.83       0.59\n"
     ]
    }
   ],
   "source": [
    "# LGBM optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = LGBMClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'num_leaves': scope.int(hp.quniform('num_leaves', 2, 32, 2)),  # Small range to avoid overfitting\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 5, 50, 5)),  # Prevent too small leaves\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),  # 0.001 to 1.0\n",
    "    'max_depth': hp.choice('max_depth', [-1, 3, 5, 7]),  # Limit depth for small dataset\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Feature selection\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Row sampling for regularization\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -4, 1),  # L1 regularization (0.0001 to 10)\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -4, 1),  # L2 regularization (0.0001 to 10)\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Limit estimators for small dataset\n",
    "    'boosting_type': hp.choice('boosting_type', ['gbdt', 'dart', 'rf']),  # Try both methods\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['num_leaves'] = int(best_params['num_leaves'])\n",
    "best_params['min_data_in_leaf'] = int(best_params['min_data_in_leaf'])\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = LGBMClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "# accuracy = final_model.score(x_test_subset,y_test)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "y_pred = clf.predict(x_test_subset)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "metrics = []\n",
    "metrics.append([accuracy, recall, precision])\n",
    "\n",
    "# importances.append(clf.feature_importances_)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "s = sns.heatmap(cm, ax = ax[idx], cbar = False, annot=True, cmap='Blues')\n",
    "s.set_xlabel('Pred')\n",
    "s.set_ylabel('True')\n",
    "s.set_title(clf_name)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics, columns=['Accuracy', 'Recall', 'Precision'])\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:00<01:57,  4.22trial/s, best loss: -0.5333333333333333]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: \n",
      "All the 3 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1531, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n",
      "    _check_call(\n",
      "  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: value -1 for Parameter max_depth should be greater equal to 0\n",
      "max_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [00:00<02:04,  3.99trial/s, best loss: -0.5333333333333333]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1531, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n    bst.update(dtrain, iteration=i, fobj=obj)\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n    _check_call(\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: value -1 for Parameter max_depth should be greater equal to 0\nmax_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m search_space \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m0\u001b[39m),  \u001b[38;5;66;03m# ~0.0001 to 1\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m9\u001b[39m]),  \u001b[38;5;66;03m# Integer selection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m),  \u001b[38;5;66;03m# L2 regularization ~0.0067 to 7.39\u001b[39;00m\n\u001b[0;32m     22\u001b[0m }\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n\u001b[0;32m     28\u001b[0m best_params \u001b[38;5;241m=\u001b[39m space_eval(space, best_params)\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[68], line 10\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(params):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#print(f\"Trying params: {params}\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model \u001b[38;5;241m=\u001b[39m XGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_scorer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39mscore, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:450\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    429\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    430\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    431\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    432\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    448\u001b[0m )\n\u001b[1;32m--> 450\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32md:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 3 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1531, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 726, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n    bst.update(dtrain, iteration=i, fobj=obj)\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 2100, in update\n    _check_call(\n  File \"d:\\Installations\\Python\\Lib\\site-packages\\xgboost\\core.py\", line 284, in _check_call\n    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\nxgboost.core.XGBoostError: value -1 for Parameter max_depth should be greater equal to 0\nmax_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy\n"
     ]
    }
   ],
   "source": [
    "# XGBoost optimization with HyperOpt\n",
    "\n",
    "X_subset = df[features_subset[1]]\n",
    "x_train_subset, x_test_subset, y_train, y_test = train_test_split(X_subset, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    #print(f\"Trying params: {params}\")\n",
    "    model = XGBClassifier(**params, verbose=-1)\n",
    "    score = cross_val_score(model, x_train_subset, y_train, cv=3, scoring=my_scorer).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -4, 0),  # ~0.0001 to 1\n",
    "    'max_depth': hp.choice('max_depth', [2, 3, 4, 5, 6, 7, 8, 9]),  # Integer selection\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),  # Integer 50-300 in steps of 10\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),  # Use 50-100% of data per round\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),  # Use 50-100% of features per tree\n",
    "    'gamma': hp.loguniform('gamma', -5, 1),  # ~0.0067 to 2.71\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, 1),  # L1 regularization ~0.0067 to 2.71\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -5, 2),  # L2 regularization ~0.0067 to 7.39\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "best_params = fmin(objective, space, algo=tpe.suggest, max_evals=500)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = space_eval(space, best_params)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "final_model = XGBClassifier(**best_params, random_state=seed)\n",
    "final_model.fit(x_train_subset, y_train)\n",
    "accuracy = final_model.score(x_test_subset,y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.estimator_.get_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
